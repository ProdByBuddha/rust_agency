{
  "models": [
    {
      "name": "llama3.2:1b",
      "repo": "bartowski/Llama-3.2-1B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "meta-llama/Llama-3.2-1B-Instruct",
      "is_quantized": true,
      "quant_file": "Llama-3.2-1B-Instruct-Q4_K_M.gguf",
      "description": "Llama 3.2 1B (Q4 Quantized) - Ultra-fast, low-resource inference."
    },
    {
      "name": "llama3.2:3b",
      "repo": "bartowski/Llama-3.2-3B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "meta-llama/Llama-3.2-3B-Instruct",
      "is_quantized": true,
      "quant_file": "Llama-3.2-3B-Instruct-Q4_K_M.gguf",
      "description": "Llama 3.2 3B (Q4 Quantized) - The standard balanced model."
    },
    {
      "name": "qwen2.5:3b-q4",
      "repo": "Qwen/Qwen2.5-3B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "Qwen/Qwen2.5-3B-Instruct",
      "is_quantized": true,
      "quant_file": "qwen2.5-3b-instruct-q4_k_m.gguf",
      "description": "Qwen 2.5 3B (Q4 Quantized) - High performance alternative to Llama 3.2."
    },
    {
      "name": "qwen2.5:7b-q4",
      "repo": "Qwen/Qwen2.5-7B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "Qwen/Qwen2.5-7B-Instruct",
      "is_quantized": true,
      "quant_file": "qwen2.5-7b-instruct-q4_k_m.gguf",
      "description": "Qwen 2.5 7B (Q4 Quantized) - Sharp reasoning, outperforms Llama 3.1 8B."
    },
    {
      "name": "qwen2.5:14b-q4",
      "repo": "Qwen/Qwen2.5-14B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "Qwen/Qwen2.5-14B-Instruct",
      "is_quantized": true,
      "quant_file": "qwen2.5-14b-instruct-q4_k_m.gguf",
      "description": "Qwen 2.5 14B (Q4 Quantized) - The 'Intelligence Ceiling' for 16GB RAM."
    },
    {
      "name": "llama3.1:8b",
      "repo": "MaziyarPanahi/Meta-Llama-3.1-8B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "meta-llama/Llama-3.2-3B-Instruct",
      "is_quantized": true,
      "quant_file": "Meta-Llama-3.1-8B-Instruct.Q4_K_M.gguf",
      "description": "Llama 3.1 8B (Q4 Quantized) - Reliable long-context (128k) workhorse."
    },
    {
      "name": "qwen2.5-coder:7b-q4",
      "repo": "Qwen/Qwen2.5-Coder-7B-Instruct-GGUF",
      "revision": "main",
      "tokenizer_repo": "Qwen/Qwen2.5-Coder-7B-Instruct",
      "is_quantized": true,
      "quant_file": "qwen2.5-coder-7b-instruct-q4_k_m.gguf",
      "description": "Qwen 2.5 Coder 7B (Q4 Quantized) - State-of-the-art open coding model."
    },
    {
      "name": "qwen2.5-coder:0.5b",
      "repo": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
      "revision": "main",
      "tokenizer_repo": "Qwen/Qwen2.5-Coder-0.5B-Instruct",
      "is_quantized": false,
      "quant_file": null,
      "description": "Qwen 2.5 Coder 0.5B (F16) - Tiny specialized coding model in full precision."
    },
    {
      "name": "moondream2",
      "repo": "santiagomed/candle-moondream",
      "revision": "main",
      "tokenizer_repo": "vikhyatk/moondream2",
      "is_quantized": true,
      "quant_file": "model-q4_0.gguf",
      "description": "Moondream2 (Quantized) - Lightweight vision-language model for image understanding."
    }
  ],
  "defaults": {
    "tiny": "qwen2.5-coder:0.5b",
    "standard": "qwen2.5:3b-q4",
    "heavy": "qwen2.5:7b-q4",
    "coder": "qwen2.5-coder:7b-q4"
  }
}
